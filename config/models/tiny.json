{
  "training": {
    "patch_sizes": [
      1,
      2,
      3,
      4,
      5,
      6,
      7,
      8
    ],
    "max_lr": 0.003,
    "num_epochs": 500,
    "batch_size": 128,
    "effective_batch_size": 512,
    "warmup_epochs": 30,
    "final_lr": 1e-06,
    "weight_decay": 0.01,
    "grad_clip": true,
    "betas": [
      0.9,
      0.999
    ],
    "ema": [
      0.996,
      1.0
    ],
    "shape_time_combinations": [
      {
        "size": 4,
        "timesteps": 12
      },
      {
        "size": 5,
        "timesteps": 6
      },
      {
        "size": 6,
        "timesteps": 4
      },
      {
        "size": 7,
        "timesteps": 3
      },
      {
        "size": 9,
        "timesteps": 3
      },
      {
        "size": 12,
        "timesteps": 3
      }
    ],
    "masking_probabilities": [
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0,
      1.0
    ],
    "augmentation": {
      "flip+rotate": true
    },
    "encode_ratio": 0.1,
    "decode_ratio": 0.5,
    "max_unmasking_channels": 17,
    "loss_type": "patch_disc",
    "tau": 0.1,
    "pred2unit": false,
    "loss_mask_other_samples": true,
    "normalization": "std",
    "eval_eurosat_every_n_epochs": 10,
    "random_masking": "half",
    "unmasking_channels_combo": "all",
    "double_loss": true,
    "double_predictors": true,
    "target_exit_after": 0,
    "token_exit_cfg": {
      "S1": 12,
      "S2_RGB": 12,
      "S2_Red_Edge": 12,
      "S2_NIR_10m": 12,
      "S2_NIR_20m": 12,
      "S2_SWIR": 12,
      "NDVI": 6,
      "ERA5": 6,
      "TC": 6,
      "VIIRS": 12,
      "SRTM": 6,
      "DW": 0,
      "WC": 0,
      "LS": 0,
      "location": 12,
      "DW_static": 0,
      "WC_static": 0
    },
    "target_masking": "all"
  },
  "model": {
    "encoder": {
      "embedding_size": 192,
      "depth": 12,
      "num_heads": 3,
      "mlp_ratio": 4,
      "max_sequence_length": 24,
      "freeze_projections": false,
      "drop_path": 0.1,
      "max_patch_size": 8
    },
    "decoder": {
      "depth": 4,
      "num_heads": 3,
      "mlp_ratio": 4,
      "max_sequence_length": 24,
      "learnable_channel_embeddings": true,
      "max_patch_size": 8,
      "embedding_size": 192
    }
  }
}